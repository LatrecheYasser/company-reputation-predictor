{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import ast\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "from utils import CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preprocess(filename):\n",
    "    df = pandas.read_csv(\"intermediate.csv\", delimiter=\"#\", encoding=\"utf-8\")\n",
    "    with open(filename, \"w+\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(\"topic#text\\n\")\n",
    "        for i in range(len(df)):\n",
    "            topic = df[\"topic\"][i]\n",
    "\n",
    "            stems = ast.literal_eval(df[\"text\"][i])\n",
    "            stems = stems[:100]\n",
    "            stems.insert(0,'[CLS]')\n",
    "            stems.append('[SEP]')\n",
    "\n",
    "            file.writelines(str(topic) + \"#\" + str(stems) + \"\\n\")\n",
    "\n",
    "def final_preprocess2(filename):\n",
    "    df = pandas.read_csv(\"filtered.csv\", delimiter=\"#\", encoding=\"utf-8\")\n",
    "    with open(filename, \"w+\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(\"topic#text\\n\")\n",
    "        for i in range(len(df)):\n",
    "            topic = df[\"topic\"][i]\n",
    "\n",
    "            stems = ast.literal_eval(df[\"text\"][i])\n",
    "            stems.insert(0,'[CLS]')\n",
    "            stems.append('[SEP]')\n",
    "\n",
    "            file.writelines(str(topic) + \"#\" + str(stems) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"final.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file.readline()\n",
    "    for line in file:\n",
    "        res = ast.literal_eval(line.split(\"#\")[1][:-1])\n",
    "        res = ' '.join(res)\n",
    "        sentences.append(res)\n",
    "\n",
    "labels = []\n",
    "with open(\"final.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file.readline()\n",
    "    for line in file:\n",
    "            labels.append(int(line.split(\"#\")[0]))\n",
    "\n",
    "nb_labels = len(CLASSES)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum sequence length.\n",
    "MAX_LEN = 128\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "\n",
    "# split data into train and validation\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
    "                                                                                    random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                                       random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into tensors\n",
    "train_inputs = torch.LongTensor(train_inputs)\n",
    "validation_inputs = torch.LongTensor(validation_inputs)\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "validation_labels = torch.LongTensor(validation_labels)\n",
    "train_masks = torch.Tensor(train_masks)\n",
    "validation_masks = torch.Tensor(validation_masks)\n",
    "\n",
    "# batch size for training.\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=nb_labels)\n",
    "model.cuda()\n",
    "\n",
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# loss and accuracy\n",
    "train_loss_set = []\n",
    "# Number of training epochs\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT training\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # training mode\n",
    "    model.train()\n",
    "    \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    # Train\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        batch = tuple(t for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device), labels=b_labels.to(device))\n",
    "        train_loss_set.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "        del loss\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
    "\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()\n",
    "torch.save({\"state_dict\": model.state_dict(),\n",
    "            \"epoch\": 20},\n",
    "           \"topicClassifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in query_data_test]\n",
    "labels = intent_data_label_test\n",
    "\n",
    "# tokenize\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "MAX_LEN = 128\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention masks\n",
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tensors\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "batch_size = 32\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation mode\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# Predict\n",
    "for batch in prediction_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "  \n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "for i in range(len(true_labels)):\n",
    "    matthews = matthews_corrcoef(true_labels[i],\n",
    "                                 np.argmax(predictions[i], axis=1).flatten())\n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(\n",
    "    matthews_corrcoef(flat_true_labels, flat_predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
