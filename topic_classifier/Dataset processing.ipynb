{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import ast\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class():\n",
    "    CLASSES = set()\n",
    "    with open(\"topics.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            CLASSES.add(str(line[:-1]))\n",
    "    return list(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = load_class()\n",
    "stemmer = FrenchStemmer()\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokenizer.tokenize(text)\n",
    "    tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('french')]\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_get_classes(text):\n",
    "    labels = ast.literal_eval(text)\n",
    "    topic = []\n",
    "    for label in labels:\n",
    "        topic.append(CLASSES.index(str(label)))\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(text):\n",
    "    return CLASSES.index(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    counter = Counter()\n",
    "    df = pandas.read_csv(filename, delimiter=\"#\", encoding=\"utf-8\")\n",
    "    with open(\"intermediate.csv\", \"w+\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(\"topic#text\\n\")\n",
    "        for i in range(len(df)):\n",
    "            topic = get_classes(df[\"topic\"][i])\n",
    "\n",
    "            stems = tokenize_and_stem(df[\"text\"][i])\n",
    "            counter += Counter(stems)\n",
    "\n",
    "            file.writelines(str(topic) + \"#\" + str(stems) + \"\\n\")\n",
    "\n",
    "    vocabulary = {s: i for i, (s, c) in enumerate(counter.most_common(len(counter)))}\n",
    "\n",
    "    with open(\"vocabulary.txt\", \"w+\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(str(vocabulary) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(vocabulary, text):\n",
    "    stems = tokenize_and_stem(text)\n",
    "    indexes = [vocabulary[s] for s in stems if s in vocabulary]\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_voc():\n",
    "    with open(\"vocabulary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        voc = ast.literal_eval(file.readline())\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sectors():\n",
    "    df = pandas.read_csv(\"data.csv\", delimiter=\",\")\n",
    "    sirens = df[\"siren\"].values\n",
    "    df = None\n",
    "    df2 = pandas.read_csv(\"naf2008_5_niveaux.csv\", delimiter=\";\")\n",
    "\n",
    "    sectors = set()\n",
    "    with open(\"company_sector.csv\", \"w+\") as out:\n",
    "        out.writelines(\"siren#sector\\n\")\n",
    "        with open(\"company_referentielle.json1\", \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                item = json.loads(line, encoding=\"utf-8\")\n",
    "                if item[\"siren\"] in sirens:\n",
    "                    sir = item[\"naf_ent\"][:2]+'.'+item[\"naf_ent\"][2:]\n",
    "                    sectors.add(df2[df2[\"NIV5\"] == sir]['NIV2'].values[0])\n",
    "                    out.writelines(str(item[\"siren\"]) + '#' + str(df2[df2[\"NIV5\"] == sir]['NIV2'].values[0]) + \"\\n\")\n",
    "\n",
    "    print(len(sectors))\n",
    "    with open(\"topics.txt\", \"w+\") as f:\n",
    "        for sector in sectors:\n",
    "            f.writelines(str(sector) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering():\n",
    "    df = pandas.read_csv(\"company_sector.csv\", delimiter=\"#\", encoding=\"utf-8\")\n",
    "    sirens = df[\"siren\"].values\n",
    "    df2 = pandas.read_csv(\"data.csv\", delimiter=\",\", encoding=\"utf-8\")\n",
    "    valid = [\"70\", \"72\", \"64\", \"35\", \"71\", \"66\", \"68\", \"63\", \"62\", \"58\", \"21\", \"82\", \"46\", \"26\"]\n",
    "    with open(\"filtered.csv\", \"w+\", encoding=\"utf-8\") as out:\n",
    "        out.writelines('topic#text\\n')\n",
    "        for index, row in df2.iterrows():\n",
    "            texts = row[\"text\"]\n",
    "            sector = str(df[df[\"siren\"] == row[\"siren\"]][\"sector\"].values[0])\n",
    "            if sector in valid:\n",
    "                out.writelines(sector + \"#\" + texts.replace(\"#\", \" \").replace(\"\\n\", \" \") + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split():\n",
    "    df = pandas.read_csv(\"intermediate.csv\", delimiter=\"#\", encoding=\"utf-8\")\n",
    "    msk = np.random.rand(len(df)) < 0.95\n",
    "    train = df[msk]\n",
    "    valid = df[~msk]\n",
    "    train.to_csv(\"train.csv\")\n",
    "    valid.to_csv(\"valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset():\n",
    "    df = pandas.read_csv(\"final.csv\", delimiter=\"#\", encoding=\"utf-8\")\n",
    "    with open(\"final2.csv\", \"w+\", encoding=\"utf-8\")as out:\n",
    "        out.writelines(\"topic#text\\n\")\n",
    "        j = 0\n",
    "        for i in range(len(df)):\n",
    "            words = ast.literal_eval(df[\"text\"][i])\n",
    "            if len(words) > 500:\n",
    "                out.writelines(str(df[\"topic\"][i])+\"#\"+str(words[:500])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preprocess(fileout):\n",
    "    df = pandas.read_csv(\"intermediate.csv\", delimiter=\"#\", encoding=\"utf-8\")\n",
    "\n",
    "    vocabulary = load_voc()\n",
    "\n",
    "    with open(fileout, \"w+\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(\"topic#text\\n\")\n",
    "        for i in range(len(df)):\n",
    "            stems = ast.literal_eval(df[\"text\"][i])\n",
    "            indexes = [vocabulary[s] for s in stems]\n",
    "            file.writelines(str(df[\"topic\"][i]) + \"#\" + str(indexes) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_sectors()\n",
    "filtering()\n",
    "preprocess(\"filtered.csv\")\n",
    "final_preprocess(\"final.csv\")\n",
    "normalize_dataset()\n",
    "split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats():\n",
    "    results = {}\n",
    "    with open(\"filtered.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line[:-1] not in results:\n",
    "                results[line[:-1]] = 1\n",
    "            else:\n",
    "                results[line[:-1]] = results[line[:-1]] + 1\n",
    "    for key, value in results.items():\n",
    "        if value > 99:\n",
    "            print(str(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
