{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import unidecode\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FORMULE_PATH = '../data/output/secondFormule/'\n",
    "ARTICLES_PATH = '../data/input/articles.csv'\n",
    "OUTPUT_DIST_PATH = '../data/output/'\n",
    "'''\n",
    "Function that reconstructs a dictionary of dataframes that was divided into a file with the keys and\n",
    "a csv file for each dataframe of the corresponding key.\n",
    "Returns: A dictionary composed of the keys and dataframes read from files of format key.txt and \n",
    "data_'key value'.csv\n",
    "'''\n",
    "def loader():\n",
    "    with open(OUTPUT_FORMULE_PATH + 'keys.txt', 'r') as f:\n",
    "        keys = eval(f.read())\n",
    "        \n",
    "    dictex = {}\n",
    "    for key in keys:\n",
    "        dictex[key] = pd.read_csv(OUTPUT_FORMULE_PATH + \"data_{}.csv\".format(str(key)))\n",
    "    return dictex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/output/secondFormule/keys.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-82ceda7808da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorpuses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARTICLES_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcorpuses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpuses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-090d2a70b5eb>\u001b[0m in \u001b[0;36mloader\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m '''\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_FORMULE_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'keys.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/output/secondFormule/keys.txt'"
     ]
    }
   ],
   "source": [
    "dict_res = loader()\n",
    "corpuses = pd.read_csv(ARTICLES_PATH, keep_default_na=False)\n",
    "corpuses['corpus'] = corpuses.apply(lambda x:'%s\\n%s' % (x['title'],x['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function that search the corpuses that contain the listed ngrams.\n",
    "Params:\n",
    "    corpuses_list: A list of the corpuses where to search the presence of the ngrams.\n",
    "    \n",
    "    ngram_list: A list of the ngrams to search in the corpuses.\n",
    "Returns:\n",
    "    matchs: A list of the indices of the corpuses where at least one of the ngrams appear.'''\n",
    "def search(corpuses_list, ngram_list):\n",
    "    matchs = set()\n",
    "    for index, corpus in enumerate(corpuses_list):\n",
    "        corpus = unidecode.unidecode(corpus)\n",
    "        for ngram in ngram_list:\n",
    "            #Warning, corpus data had to be pretreited to avoid problems with accents\n",
    "            pat = r'\\b'+ngram+r'\\b'\n",
    "            if re.search(pat, corpus, flags=re.I) != None:\n",
    "                matchs.add(index)\n",
    "                break\n",
    "    return matchs\n",
    "\n",
    "#Compute distance for each combination of parameters\n",
    "'''\n",
    "Function that compute the distance for each combinations of parameters, given a specific threshold. It saves\n",
    "in a file named distances.json the calculated distances in a progressive way, so if it is stopped in the middle\n",
    "of computation, the already measured distances shouldn't be lost.\n",
    "Params:\n",
    "    corpuses: A dataframe that contains the used corpuses (title + text of articles) and the company with\n",
    "              which it is linked according to ground-truth.\n",
    "    combinations: A dictionary structure that contains the values of the weights of the pertinence function\n",
    "                  and the dataframe with all the calculated pertinences.\n",
    "    threshold: The value of the threshold from which to take into account the ngrams. The scores that are above\n",
    "               are considered in the distance calculation, those who are below or equal are excluded from computing.\n",
    "Returns:\n",
    "    res: A dictionary that contains a list of the weight combinations evaluated, and the list of corresponding\n",
    "         measured accuracies.\n",
    "'''\n",
    "def get_distance_all(corpuses, combinations, treshold):\n",
    "    #Compute distance as accuracy for each combination of weights\n",
    "    res = {'weights': [], 'accuracy': []}\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    mlb.fit([corpuses.index.values])\n",
    "    enterprises = corpuses['name'].unique().tolist()\n",
    "    for key, df in combinations.items():\n",
    "        founds = []\n",
    "        truths = []\n",
    "        for company in enterprises:\n",
    "            #Compute ground truth of corpuses for one company\n",
    "            truths.append(corpuses.loc[corpuses['name'] == company].index.values)\n",
    "            #Filter only pertinent ngrams\n",
    "            ngrams = df.loc[(df['pertinence'] > treshold) & (df['name'] == company)]['ngram'].tolist()\n",
    "            #What is what we get\n",
    "            founds.append(search(corpuses['corpus'].tolist(), ngrams))\n",
    "            \n",
    "        y_correct = mlb.transform(truths)\n",
    "        y_pred = mlb.transform(founds)\n",
    "        res['weights'].append(key)\n",
    "        #Accuray agaisnt ground truth\n",
    "        res['accuracy'].append(accuracy_score(y_correct, y_pred, normalize=True))\n",
    "        with open(OUTPUT_DIST_PATH + 'distances.json', 'w') as file:\n",
    "            file.write(json.dumps(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of combination of keys that we are going to test\n",
    "required_fields = ['0.2,0.4,0.4']\n",
    "dict_eval = {key:value for key, value in dict_res.items() if key in required_fields}\n",
    "#Eval\n",
    "distances = get_distance_all(corpuses, dict_eval, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "corpus_t = pd.DataFrame()\n",
    "corpus_t['corpus'] = ['IPSEN n\\'est pas', 'Ipsadoil y a', 'Voil√† ipsen']\n",
    "corpus_t['name'] = ['IPSEN', 'x', 'x']\n",
    "df_t = pd.DataFrame()\n",
    "df_t['pertinence'] = [0.9]\n",
    "df_t['name'] = ['IPSEN']\n",
    "df_t['ngram'] = ['IPSEN']\n",
    "treshold_t = 0.5\n",
    "combination_t = {'xyz': df_t}\n",
    "res_t = get_distance_all(corpus_t, combination_t, treshold_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
